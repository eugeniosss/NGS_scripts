### CONFIG

#INPUT
DIC_FILE="dic.txt"
#FOLDER TO STORE ADAPTERREMOVAL2 OUTPUT
AR="processed_A"
#REFERENCE GENOME
REF="/media/jbod2/eugenio/ref_genomes/Oryctolagus_cuniculus.OryCun2.0.dna.toplevel.fa"
#FOLDER TO STORE UNMERGED BAMS (ONE SAMPLE=MULTIPLE BAMS)
BAMS_UN="bams_genome_mem_u"
#FOLDER TO STORE UNMERGED BAMS (ONE SAMPLE=MULTIPLE BAMS) AFTER DUPS REMOVAL
BAMS_UN_PIC="bams_genome_mem_u_pic/"
#FOLDER TO STORE MERGED BAMS (ONE SAMPLE=ONE BAM)
BAMS="bams_genome_mem"
#FOLDER TO STORE MERGED BAMS (ONE SAMPLE=ONE BAM) AFTER DUPS REMOVAL
BAMS_PIC="bams_genome_mem_pic"
#FOLDER TO STORE MT FASTA
FASTAS="fastas_MT"
#FOLDER TO STORE STATS PER RUN AND PER SAMPLE
STATS="stats"
#PATH TO MT BED! REMINDEEEEERRR: MITOCHONDRIAL GENOME IN REF, MUST BE ASSIGNED AS MT!!! (IE ">MT")
MT_BED="/media/jbod2/eugenio/ref_genomes/Oryctolagus_cuniculus.OryCun2.0.dna.toplevel_MT.bed"
#PATH TO NUCLEAR (OR EXOME, OR TARGET) BED
NU_BED="/media/jbod2/eugenio/ref_genomes/IRN10000035363_Ocun_31Oct2019_primary_targets_clean_No_X_MT.bed"
#PATH TO NGS_SCRIPTS FOLDER
NGS_SCRIPTS="/media/jbod2/eugenio/NGS_scripts/"

### END OF CONFIG

import csv
import re

csvfile = csv.reader(open(DIC_FILE))
DIC = dict(csvfile)

DIC_MERGE=dict()

for SEQ in DIC.keys():
    SAMPLE=SEQ.split("_")[0]
    if SAMPLE in DIC_MERGE.keys():
        if SEQ not in DIC_MERGE[SAMPLE]:
            DIC_MERGE[SAMPLE]=DIC_MERGE[SAMPLE]+" "+BAMS_UN_PIC+"/"+SEQ+".bam"
    else:
        DIC_MERGE[SAMPLE]=BAMS_UN_PIC+"/"+SEQ+".bam"

wildcard_constraints:
    SAMPLE=".{4,9}",
    SEQ=".{18,26}"
        
rule all:
    input:
           [BAMS_UN_PIC+"/{SEQ}.bam".format(SEQ=SEQ) for SEQ in DIC.keys()],
           "metrics_libs.csv",
           [BAMS_PIC+"/{SAMPLE}.bam".format(SAMPLE=SAMPLE) for SAMPLE in DIC_MERGE.keys()],
           "metrics_samples.csv",



### INDEX REF ###

os.path.splitext(REF)
REF_RAW=os.path.splitext(REF)[0]
REF_EXT=os.path.splitext(REF)[1]

rule generate_circualr_ref:
    input:
        REF
    output:
        REF_RAW+"_500"+REF_EXT
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    shell:  
        "circulargenerator -e 500 -i {input} -s 'MT'"

rule bwa_index:
    input:
        REF_RAW+"_500"+REF_EXT,
    output:
        idx=multiext(REF_RAW+"_500"+REF_EXT, ".amb", ".ann", ".bwt", ".pac", ".sa"),
    log:
        "bwa_index/indexing_500.log",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    params:
        extra="",
    script:
        NGS_SCRIPTS+"scripts_snake/run_bwa_index.py"

rule create_fai:
    input:
        REF_RAW+REF_EXT,
    output:
        REF_RAW+REF_EXT+".fai",
    log:
        "bwa_index/create_fai.log",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    shell:
        "(samtools faidx {input}) > {log} 2>&1"

rule create_dic:
    input:
        fasta=REF_RAW+REF_EXT,
        fai=REF_RAW+REF_EXT+".fai",
    output:
        REF_RAW+".dict",
    log:
        "bwa_index/create_dic.log",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    shell:
        "picard CreateSequenceDictionary R={input.fasta} O={output} > {log} 2>&1"

###

rule adapterremoval_pe:
    input:
       sample= lambda wildcards: DIC[wildcards.SEQ].split()
    output:
        fq1=temporary(AR+"/{SEQ}.pair1.truncated.gz"),                           # trimmed mate1<reads
        fq2=temporary(AR+"/{SEQ}.pair2.truncated.gz"),                           # trimmed mate2 reads
        collapsed=temporary(AR+"/{SEQ}.collapsed.gz"),              # overlapping mate-pairs which have been merged into a single read
        discarded=temporary(AR+"/{SEQ}.discarded.gz"),              # reads that did not pass filters
        settings=AR+"/{SEQ}.settings"                            # parameters as well as overall statistics
    log:
        AR+"/{SEQ}.log"
    params:
        adapters="",
        extra="--collapse --trimns --trimqualities --minlength 30 --preserve5p"
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    threads: 2
    script:
        NGS_SCRIPTS+"scripts_snake/run_AdapRemovl_merge.py"

rule combine_pe:
    input:
        fq1=AR+"/{SEQ}.pair1.truncated.gz",  
        fq2=AR+"/{SEQ}.pair2.truncated.gz",  
        collapsed=AR+"/{SEQ}.collapsed.gz",  
    output:
        temporary(AR+"/{SEQ}.combined.gz")
    shell: "cat {input.fq1} {input.fq2} {input.collapsed} > {output}"

rule bwa_aln:
    input:
        fastq=AR+"/{SAMPLE}_{LA}_{DATE}.combined.gz",
        idx=multiext(REF_RAW+"_500"+REF_EXT, ".amb", ".ann", ".bwt", ".pac", ".sa"),
    output:
        "tmp_{SAMPLE}_{LA}_{DATE}/{SAMPLE}_{LA}_{DATE}.sai",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    params:
        extra="-l 1024 -o 2 -n 0.01",
    log:
        BAMS_UN+"/{SAMPLE}_{LA}_{DATE}_mapping.log",
    threads: 23
    script:
        NGS_SCRIPTS+"scripts_snake/run_bwa_aln.py"

rule bwa_sam:
    input:
        fastq=AR+"/{SAMPLE}_{LA}_{DATE}.combined.gz",
        sai="tmp_{SAMPLE}_{LA}_{DATE}/{SAMPLE}_{LA}_{DATE}.sai",
        idx=multiext(REF_RAW+"_500"+REF_EXT, ".amb", ".ann", ".bwt", ".pac", ".sa"),
    output:
        "tmp_{SAMPLE}_{LA}_{DATE}/{SAMPLE}_{LA}_{DATE}.bam",
    params:
        extra=r"-r '@RG\tID:{SAMPLE}_{LA}_{DATE}\tSM:{SAMPLE}\tLB:{SAMPLE}_{LA}'",
        sort="none",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    log:
        BAMS_UN+"/{SAMPLE}_{LA}_{DATE}_sam.log",
    script:
        NGS_SCRIPTS+"scripts_snake/run_bwa_samxe.py"

rule circularmapper:
    input:
        bam="tmp_{SAMPLE}_{LA}_{DATE}/{SAMPLE}_{LA}_{DATE}.bam",
        ref=REF  
    output:
        "tmp_{SAMPLE}_{LA}_{DATE}/{SAMPLE}_{LA}_{DATE}_realigned.bam",
    log:
        BAMS_UN+"/{SAMPLE}_{LA}_{DATE}_cir.log",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    shell: "realignsamfile -i {input.bam} -e 500 -r {input.ref} > {log} 2>&1"

rule samtools_sort_libs:
    input:
        "tmp_{SAMPLE}_{LA}_{DATE}/{SAMPLE}_{LA}_{DATE}_realigned.bam",
    output:
        BAMS_UN+"/{SAMPLE}_{LA}_{DATE}.bam"
    log:
        BAMS_UN+"/{SAMPLE}_{LA}_{DATE}_sorting.log",
    params:
        extra=" -m 4G",
    threads: 1
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    script:
        NGS_SCRIPTS+"scripts_snake/run_samtools_sort.py"

rule mark_duplicates_libs:
    input:
        bams=BAMS_UN+"/{SAMPLE}_{LA}_{DATE}.bam"
    # optional to specify a list of BAMs; this has the same effect
    # of marking duplicates on separate read groups for a sample
    # and then merging
    output:
        bam=BAMS_UN_PIC+"/{SAMPLE}_{LA}_{DATE}.bam",
        metrics=BAMS_UN_PIC+"/{SAMPLE}_{LA}_{DATE}.metrics.txt",
    conda:
       NGS_SCRIPTS+"envs/NGS.yml"
    log:
        BAMS_UN_PIC+"/{SAMPLE}_{LA}_{DATE}.log",
    params:
        extra="--REMOVE_DUPLICATES true --VALIDATION_STRINGENCY LENIENT --ASSUME_SORT_ORDER coordinate",
    # optional specification of memory usage of the JVM that snakemake will respect with global
    # resource restrictions (https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#resources)
    # and which can be used to request RAM during cluster job submission as `{resources.mem_mb}`:
    # https://snakemake.readthedocs.io/en/latest/executing/cluster.html#job-properties
    resources:
        mem_mb=11000,
    script:
        NGS_SCRIPTS+"scripts_snake/run_picard_duplicates.py"

rule samtools_index_libs:
    input:
        BAMS_UN_PIC+"/{SAMPLE}_{LA}_{DATE}.bam",
    output:
        BAMS_UN_PIC+"/{SAMPLE}_{LA}_{DATE}.bam.bai",
    log:
        BAMS_UN_PIC+"/{SAMPLE}_{LA}_{DATE}_index.log",
    params:
        extra="",  # optional params string
    threads: 1  # This value - 1 will be sent to -@
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    script:
        NGS_SCRIPTS+"scripts_snake/run_samtools_index.py"

rule get_stats_libs:
    input:
        bam=BAMS_UN_PIC+"/{SEQ}.bam",
        bai=BAMS_UN_PIC+"/{SEQ}.bam.bai",
    output:
        STATS+"/{SEQ}_basic_stats.txt",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    params:
        bed=NU_BED
    log:
        STATS+"/{SEQ}_basic_stats.log",
    resources:
        mem_mb=1024,
    script:
        NGS_SCRIPTS+"scripts_snake/run_get_stats.py"

rule get_fastas_MT:
    input:
        bam=BAMS_UN_PIC+"/{SEQ}.bam",
        reference=REF,
        bed=MT_BED,
        bai=BAMS_UN_PIC+"/{SEQ}.bam.bai",
    output:
        FASTAS+"/{SEQ}.fasta",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    log:
        FASTAS+"/{SEQ}.log",
    params:
        extra="-l30 -q30 -Q30 -s5 -e -M",
    # optional specification of memory usage of the JVM that snakemake will respect with global
    # resource restrictions (https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#resources)
    # and which can be used to request RAM during cluster job submission as `{resources.mem_mb}`:
    # https://snakemake.readthedocs.io/en/latest/executing/cluster.html#job-properties
    resources:
        mem_mb=1024,
    script:
        NGS_SCRIPTS+"scripts_snake/run_htsbox_pileup.py"

rule get_coverage_MT:
    input:
        bam=BAMS_UN_PIC+"/{SEQ}.bam",
        fasta=REF,
        intervals=MT_BED,
        bai=BAMS_UN_PIC+"/{SEQ}.bam.bai",
        dic=REF_RAW+".dict",
    output:
        multiext(
            "stats/{SEQ}_MT",
            "",
            ".sample_interval_summary",
            ".sample_cumulative_coverage_counts",
            ".sample_cumulative_coverage_proportions",
            ".sample_interval_statistics",
            ".sample_statistics",
            ".sample_summary"),
    log:
        STATS+"/{SEQ}_coverage_MT.log",
    params:
        extra="--minMappingQuality 30",
        java_opts="",
    conda:
        NGS_SCRIPTS+"envs/gatk3.yml",
    resources:
        mem_mb=20000,
    script:
        NGS_SCRIPTS+"scripts_snake/run_gatk_coverage.py"

rule get_coverage_exome:
    input:
        bam=BAMS_UN_PIC+"/{SEQ}.bam",
        fasta=REF,
        intervals=NU_BED,
        bai=BAMS_UN_PIC+"/{SEQ}.bam.bai",
        dic=REF_RAW+".dict",
    output:
        multiext(
            "stats/{SEQ}_nuclear",
            "",
            ".sample_interval_summary",
            ".sample_cumulative_coverage_counts",
            ".sample_cumulative_coverage_proportions",
            ".sample_interval_statistics",
            ".sample_statistics",
            ".sample_summary"),
    log:
        STATS+"/{SEQ}_coverage_nuclear.log",
    params:
        extra="--minMappingQuality 30",
        java_opts="",
    conda:
        NGS_SCRIPTS+"envs/gatk3.yml"
    resources:
        mem_mb=20000,
    script:
        NGS_SCRIPTS+"scripts_snake/run_gatk_coverage.py"


rule get_metrics_libs:
    input:
        stats=[STATS+"/{SEQ}_basic_stats.txt".format(SEQ=SEQ) for SEQ in DIC.keys()],
        fastas=[FASTAS+"/{SEQ}.fasta".format(SEQ=SEQ) for SEQ in DIC.keys()],
        coverages_MT=[STATS+"/{SEQ}_MT".format(SEQ=SEQ) for SEQ in DIC.keys()],
        coverages_NU=[STATS+"/{SEQ}_nuclear".format(SEQ=SEQ) for SEQ in DIC.keys()],
    output:
        final="metrics_libs.csv"
    params:
        tmp="tmp_libs.txt",
        codes=list(DIC.keys()),
        stats=STATS,
        settings=AR,
        pic=BAMS_UN_PIC,
        fastas=FASTAS,
        MT_coverage=STATS,
        nuclear_coverage=STATS,
        reads_on_target="True"
    script:
        NGS_SCRIPTS+"scripts_snake/summarize_stats.py"


rule samtools_merge_sample:
    input:
        lambda wildcards: DIC_MERGE[wildcards.SAMPLE].split()
    output:
        "tmp_{SAMPLE}/{SAMPLE}.bam",
    log:
        BAMS+"/{SAMPLE}_merging.log",
#   "tmp_{SAMPLE}/{SAMPLE}.log",
    params:
        extra="--VALIDATION_STRINGENCY LENIENT",  # optional additional parameters as string
    threads: 1
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    script:
        NGS_SCRIPTS+"scripts_snake/run_picard_MergeSamFiles.py"

rule samtools_sort_samples:
    input:
        "tmp_{SAMPLE}/{SAMPLE}.bam"
    output:
        BAMS+"/{SAMPLE}.bam",
    log:
        BAMS+"/{SAMPLE}_sorting.log",
    params:
        extra="-m 4G",
    threads: 1
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    script:
        NGS_SCRIPTS+"scripts_snake/run_samtools_sort.py"

rule mark_duplicates_samples:
    input:
        bams=BAMS+"/{SAMPLE}.bam",
    # optional to specify a list of BAMs; this has the same effect
    # of marking duplicates on separate read groups for a sample
    # and then merging
    output:
        bam=BAMS_PIC+"/{SAMPLE}.bam",
        metrics=BAMS_PIC+"/{SAMPLE}.metrics.txt",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    log:
        BAMS_PIC+"/{SAMPLE}.log",
    params:
        extra="--REMOVE_DUPLICATES true --VALIDATION_STRINGENCY LENIENT --ASSUME_SORT_ORDER coordinate",
    # optional specification of memory usage of the JVM that snakemake will respect with global
    # resource restrictions (https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#resources)
    # and which can be used to request RAM during cluster job submission as `{resources.mem_mb}`:
    # https://snakemake.readthedocs.io/en/latest/executing/cluster.html#job-properties
    resources:
        mem_mb=11000,
    script:
        NGS_SCRIPTS+"scripts_snake/run_picard_duplicates.py"

rule samtools_index_samples:
    input:
        BAMS_PIC+"/{SAMPLE}.bam",
    output:
        BAMS_PIC+"/{SAMPLE}.bam.bai",
    log:
        BAMS_PIC+"/{SAMPLE}_index.log",
    params:
        extra="",  # optional params string
    threads: 1  # This value - 1 will be sent to -@
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    script:
        NGS_SCRIPTS+"scripts_snake/run_samtools_index.py"

rule get_fastas:
    input:
        bam=BAMS_PIC+"/{SAMPLE}.bam",
        bai=BAMS_PIC+"/{SAMPLE}.bam.bai",
        reference=REF,
        bed=MT_BED
    output:
        FASTAS+"/{SAMPLE}.fasta",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    log:
        FASTAS+"/{SAMPLE}.log",
    params:
        extra="-l30 -q30 -Q30 -s5 -e -M",
    # optional specification of memory usage of the JVM that snakemake will respect with global
    # resource restrictions (https://snakemake.readthedocs.io/en/latest/snakefiles/rules.html#resources)
    # and which can be used to request RAM during cluster job submission as `{resources.mem_mb}`:
    # https://snakemake.readthedocs.io/en/latest/executing/cluster.html#job-properties
    resources:
        mem_mb=1024,
    script:
        NGS_SCRIPTS+"scripts_snake/run_htsbox_pileup.py"

rule gatk3_coverage_MT_sample:
    input:
        bam=BAMS_PIC+"/{SAMPLE}.bam",
        fasta=REF,
        intervals=MT_BED,
        bai=BAMS_PIC+"/{SAMPLE}.bam.bai",
        dic=REF_RAW+".dict",
    output:
        file="stats/{SAMPLE}_MT",
        coverage_counts="stats/{SAMPLE}_MT.sample_cumulative_coverage_counts",
        coverage_proportions="stats/{SAMPLE}_MT.sample_cumulative_coverage_proportions",
        interval_statistics="stats/{SAMPLE}_MT.sample_interval_statistics",
        interval_summary="stats/{SAMPLE}_MT.sample_interval_summary",
        statistics="stats/{SAMPLE}_MT.sample_statistics",
        summary="stats/{SAMPLE}_MT.sample_summary",
    params:
#        extra="--minMappingQuality 30 --omitDepthOutputAtEachBase --omitIntervalStatistics"
        extra="--minMappingQuality 30"
    conda:
        NGS_SCRIPTS+"envs/gatk3.yml"
    resources:
        mem_mb=20000,
    log:
        "stats/{SAMPLE}_gatk3_converage.log"
    script:
        NGS_SCRIPTS+"scripts_snake/run_gatk_coverage.py"

rule get_coverage_exome_sample:
    input:
        bam=BAMS_PIC+"/{SAMPLE}.bam",
        fasta=REF,
        intervals=NU_BED,
        bai=BAMS_PIC+"/{SAMPLE}.bam.bai",
        dic=REF_RAW+".dict",
    output:
        multiext(
            "stats/{SAMPLE}_nuclear",
            "",
            ".sample_interval_summary",
            ".sample_cumulative_coverage_counts",
            ".sample_cumulative_coverage_proportions",
            ".sample_interval_statistics",
            ".sample_statistics",
            ".sample_summary"),
    log:
        STATS+"/{SAMPLE}_coverage_nuclear.log",
    params:
        extra="--minMappingQuality 30",
        java_opts="",
    conda:
        NGS_SCRIPTS+"envs/gatk3.yml"
    resources:
        mem_mb=20000,
    script:
        NGS_SCRIPTS+"scripts_snake/run_gatk_coverage.py"

rule get_stats_samples:
    input:
        bam=BAMS_PIC+"/{SAMPLE}.bam",
        bai=BAMS_PIC+"/{SAMPLE}.bam.bai",
    output:
        STATS+"/{SAMPLE}_basic_stats.txt",
    conda:
        NGS_SCRIPTS+"envs/NGS.yml"
    params:
        bed=NU_BED
    log:
        STATS+"/{SAMPLE}_basic_stats.log",
    resources:
        mem_mb=1024,
    script:
        NGS_SCRIPTS+"scripts_snake/run_get_stats.py"     

rule get_metrics_samples:
    input:
        stats=[STATS+"/{SAMPLE}_basic_stats.txt".format(SAMPLE=SAMPLE) for SAMPLE in DIC_MERGE.keys()],
        fastas=[FASTAS+"/{SAMPLE}.log".format(SAMPLE=SAMPLE) for SAMPLE in DIC_MERGE.keys()],
        coverages_MT=[STATS+"/{SAMPLE}_MT".format(SAMPLE=SAMPLE) for SAMPLE in DIC_MERGE.keys()],
        coverages_NU=[STATS+"/{SAMPLE}_nuclear".format(SAMPLE=SAMPLE) for SAMPLE in DIC_MERGE.keys()],
    output:
        final="metrics_samples.csv"
    params:
        tmp="tmp_samples.txt",
        codes=list(DIC_MERGE.keys()),
        stats=STATS,
        settings="False",
        pic=BAMS_PIC,
        fastas=FASTAS,
        MT_coverage=STATS,
        nuclear_coverage=STATS,
        reads_on_target="True"
    script:
        NGS_SCRIPTS+"scripts_snake/summarize_stats.py"
